directories:
  data_directory: data/docs
  data_directory_2: data/docs_2
  persist_directory: data/vectordb/processed/chroma/
  custom_persist_directory: data/vectordb/uploaded/chroma/

embedding_model_config:
  engine: "text-embedding-ada-002"

llm_config:
    llm_system_role: "You are a very insightful and helpful chatbot. You'll receive a prompt that includes a chat 
    history, retrieved content from the vectorDB based on the user's question, and the source. 
    Your task is to respond to the user's new question using the information from the vectorDB without relying on 
    your own knowledge. Do not use your own knowledge or visit the internet. Also, if you do not know the answer, please 
    just tell me that you do not know and that you need to do further research. 
    You will receive a prompt with the the following format:

    # Chat history:\n
    [user query, response]\n\n

    # Retrieved content number:\n
    Content\n\n
    Source\n\n

    # User question:\n
    New question
    "
#    engine: "gpt-3.5-turbo"
    engine: "gpt-4o-mini"
    temperature: 0.0
    max_token: 4096

summarizer_config:
    max_final_token: 3000

    character_overlap: 100
    token_threshold: 0
    summarizer_llm_system_role: "You are an expert text summarizer. You will receive a text and your task is to summarize 
    and keep all the key information. Keep the maximum length of summary within {} number of tokens."
    final_summarizer_llm_system_role: "You are an expert text summarizer. You will receive a text and your task is to 
    give a comprehensive summary and keep all the key information."


splitter_config:
  # Adjusting the chunk size value affects batch size during the embedding step.
  # Higher values equal fewer chunks and lower values equal more chunks.
  # Larger chunk_size: Fewer chunks, meaning larger pieces of text are passed into the model at once. This can help maintain better context within each chunk but might be less efficient for very large documents.
  # Smaller chunk_size: More chunks, meaning shorter sections are passed into the model. This might help with granularity but can cause loss of context between chunks.
  chunk_size: 2500
  # Larger Overlap: Helps maintain context between chunks, especially for tasks like retrieval, where understanding the continuity between parts of a document is important. However, it may lead to redundancy and increased processing time.
  # Smaller Overlap: Less redundancy and faster processing, but you risk losing critical information that spans across chunk boundaries.
  chunk_overlap: 750

retrieval_config:
  k: 4

serve:
  port: 8000

memory:
  number_of_q_a_pairs: 3


  
